# This file sets up the experiment variables for the Burgers' end state control problem.

using Problems # problem (hyper)parameters
using General # contains level mapping functions etc
using Gradient
using Stoch
import Burgers_Solver
import MLMC

prob = problems3[16]
Solver = Burgers_Solver

# setup variables
kscale = 1e-3
mx,mt = prob.m0
T = prob.T

max_value_in_u = 5.0
#max_value_in_u = (Î”x^2 - 2k*Î”t)/(Î”t*Î”x)

# Construct Hierarchy h
h = Hierarchy(RegularGrid(mx,mt))
extend!(h,prob.L,prob.a)

# Defining a ComputeStruct
getmx(â„“::Int) = (mx-1)*2^â„“+1
function cost_grad_state(u::Vector{Float64}, k::Vector{Float64})
    m = size(u,1)
    Î”x = 1/(m-1)
    Î”t = T/(mt-1)
    nodes = LinRange(0,1,m)
    z = prob.zfun.(nodes)
    Solver.cost_grad_state(u, z, k, ones(m).*prob.s, mt, Î”x, Î”t, prob.Î±)
end
h_u = Hierarchy(RegularGrid1D(mx),prob.L);
function k_sampler_generator(seed::Int)
    k_sampler = Stoch.gen_sampler(seed, prob.distribution, h_u)
    return (â„“::Int,i::Int)->kscale.*k_sampler(â„“,i)
end
restrict_k = General.inject
lm_u(u,â„“) = General.lm(u, getmx(â„“))
lm_y(y,â„“) = General.lm(y, getmx(â„“), mt)
lm_âˆ‡J(âˆ‡J,â„“) = General.lm(âˆ‡J, getmx(â„“))
costs = [2^â„“ for â„“ in 0:prob.L]
c = ComputeStruct(cost_grad_state, k_sampler_generator, restrict_k, lm_u, lm_y, lm_âˆ‡J, prob.L, costs, MLMC.AggregateRMSE())
norm_âˆ‡J(g) = norm(g,h_u.meshes[end])
inner_âˆ‡J(a,b) = inner(a,b,h_u.meshes[end])
#s_bound(u,d) = (max_value_in_u - maximum(abs.(u)))/maximum(abs.(d))
function s_bound(u,d)
    all(u.<=max_value_in_u) || @warn("maximum value in u surpassed!")
    f(u,d,a) = d==0 ? Inf : (d>0 ? (a-u)/d : (-a-u)/d)
    return minimum(f.(u,d,max_value_in_u))
end
ls_options = (method="quadmin",s_min=0.0,s_bound=s_bound)
#ls_options = (method="quadmin",s_min=0.0,s_bound=s_bound)

# For MG/OPT
cs = [ComputeStruct(cost_grad_state, k_sampler_generator, restrict_k, lm_u, lm_y, lm_âˆ‡J, â„“, costs[1:â„“+1], MLMC.AggregateRMSE()) for â„“ in 0:prob.L]
lm_mgopt = lm_u
#ls_options_smoother = (method="armijo",inner=inner_âˆ‡J,s_guess=8.0,s_decay=0.25,print=1,max_evals=10)
ls_options_smoother = (method="quadmin",s_min=0.0,s_max=10.0,s_bound=s_bound)
ls_options_smoother_fallback = (method="armijo",inner=inner_âˆ‡J,s_guess=8.0,s_decay=0.25,print=0,max_evals=10,s_bound=s_bound)
smoother(f,u,Î¼,k) = Optimization.smooth(f, u, Î¼; norm_âˆ‡J=norm_âˆ‡J, print=2, breakcond=g->norm_âˆ‡J(g)<1e-10, ls_options=ls_options_smoother, ls_options_fallback=ls_options_smoother_fallback)

#ls_options_mgopt = (method="constant",stepsize=1.0) # cheapest
ls_options_mgopt = (method="armijo",inner=inner_âˆ‡J,s_guess=1.0,s_decay=0.5) # This ensures descent happens in the MG/OPT iteration, providing the method with some globalization properties
#ls_options_mgopt = (method="quadmin",); # potentially fastest descent

# number of NCG iterations given MG/OPT level k
Î¼_pre = 2*[(2^(prob.L-k) for k in 1:prob.L)...,0]; Î¼_post = 2*[(2^(prob.L-k) for k in 1:prob.L)...,1]
#Î¼_pre = [fill(2,prob.L)...,0]; Î¼_post = fill(2,prob.L+1)

# initial guess
u0 = zeros(getmx(prob.L))

# inspection functions
s1 = SamplingData(0,fill(1,prob.L+1))
function plot_solution(J,âˆ‡J,ð”¼y,ð•y,last)
    m_y = h.meshes[end]
    m_u = h_u.meshes[end]
    println("J = $J, â€–âˆ‡Jâ€– = $(norm_âˆ‡J(âˆ‡J))")
    pp.newfig(1); pp.plot(m_u,u,1)
    pp.newfig(2); pp.plot(m_u,âˆ‡J,2)
    pp.newfig(3); pp.surf(m_y,ð”¼y,3); pp.newfig(6); pp.plot(m_y,ð”¼y,6); pp.colorbar()
    pp.newfig(4); pp.surf(m_y,ð•y,4)
    pp.newfig(5); pp.plot(m_u,ð”¼y[:,end],5); pp.plot(m_u,[prob.zfun(p.x) for p in m_u])
end

# generates a dict for exporting
function export_solution(J,âˆ‡J,ð”¼y,ð•y,Ïµ)
    m_y = h.meshes[end]
    m_u = h_u.meshes[end]
    data = Dict("u"=>u,
    "J"=>J,"grad"=>âˆ‡J,"nodes"=>m_u.nodes_x,"nodes1"=>m_y.nodes_x,"nodes2"=>m_y.nodes_y,
    "Ey"=>ð”¼y,"Vy"=>ð•y,"eps"=>Ïµ)
    toMatlab(data, "sol_B")
end


# Experimental code
# function inject_modified(v::Vector, â„“::Int, â„“_new::Int)
#     â„“_new <= â„“ || @error("Return level must be smaller than the original level.")
#     q = 2^(â„“-â„“_new)
#     v_new = v[1:q:end]
#     rate = sqrt(2)
#     v_new[1] = rate^(â„“-â„“_new)*v[1]; v_new[end] = rate^(â„“-â„“_new)*v[end];
#     boundary_multiplier = sqrt(2)^(prob.L-â„“_new)
#     v_new[2] = rate^(prob.L-â„“_new)*v[1+q]; v_new[end-1] = rate^(prob.L-â„“_new)*v[end-q];
#     return v_new
# end
# restrict_k = inject_modified

module Optimization

using General
using MLMC
using Gradient
using Problems
using Printf
using LinearAlgebra
import Plotter
pp = Plotter

export getdir, quadmin, armijo, linesearch
export ncg
export gen_f, mgoptV, mgoptW, smooth

#############################
## Essential functionality ##
#############################
# Produces Vector{Pair} containing only those s=>options.s for the s in keywords that have a corresponding options.s.
function extractKwargs(object, keywords::Vector{Symbol})
    props = propertynames(object)
    kwargs = Pair[]
    for s in keywords
        s in props && push!(kwargs, s=>getproperty(object,s))
    end
    return kwargs
end

####################
## Line Searching ##
####################
"""
    getdir(âˆ‡J, âˆ‡Jâ‚€, dâ‚€)

Produces a search direction using the Dai-Yuan formula given current gradient âˆ‡J, previous gradient âˆ‡Jâ‚€ and previous search direction dâ‚€."""
function getdir(âˆ‡J, âˆ‡Jâ‚€, dâ‚€)
    Î”g = âˆ‡J-âˆ‡Jâ‚€
    if norm(Î”g)/norm(âˆ‡J)<1e-8 # may happen if step size is very small
        # @warn("extreme values detected in getdir. Returning -âˆ‡J.")
        return -âˆ‡J
    else
        Î² = norm(âˆ‡J)^2/(dâ‚€ â‹… Î”g) # Dai-Yuan formula
        if isnan(Î²) # could happens if step size was zero. Normally should never happen.
            error("NaN detected for Î². Returning -âˆ‡J.")
            return -âˆ‡J
        end
    end
    return -âˆ‡J+Î²*dâ‚€
end

"""
    quadmin(u, g, d, âˆ‡J; s_guess=1.0, s_min=-Inf, s_max=Inf, print=0)

Finds s such that âˆ‡J(u+s*d) = 0.
g must be equal to âˆ‡J(u) and is very likely calculated beforehand. This argument exists such that it does not have to be recalculated inside this function.

Constructs a linear model âˆ‡Jm of the provided gradient âˆ‡J by interpolating the given g=âˆ‡J(u) and gâ‚ = âˆ‡J(u+s_guess*d). If âˆ‡J:uâ†¦âˆ‡J(u) provides the gradient of some quadratic cost function J, i.e., if âˆ‡J is linear in u, then this model âˆ‡Jm is exactly âˆ‡J.

Calculates the step size sâ‚€ such that âˆ‡Jm(u+sâ‚€*d)=0. Let s be the number in [s_min, s_max] closest to sâ‚€.
`quadmin` returns the step size s, and the model gradient âˆ‡Jm(u+s*d). Note that âˆ‡Jm(u+s*d) can be calculated from the two interpolation points g and gâ‚ as s*gâ‚+(1-s)*g. The gradient function âˆ‡J is therefore only called once in total. If âˆ‡J is the gradient of some non-quadratic J, the resulting âˆ‡Jm(u+s*d) can be considered a first estimate of âˆ‡J(u+s*d). Setting print larger than 0 provides more information during the calculation.

For convenience, one may put all, or a subset of, the named arguments above into a struct or named tuple instead.
"""
function quadmin(u, g, d, âˆ‡J; s_guess::Real=1.0, s_min::Real=-Inf, s_max::Real=Inf, print::Int=0)
    uâ‚ = u+s_guess*d
    gâ‚ = âˆ‡J(uâ‚)
    c = g â‹… d # directional derivative if norm(d) = 1
    câ‚ = gâ‚ â‹… d # directional derivative if norm(d) = 1
    s = c/(c-câ‚) # c+s*(câ‚-c) = 0
    s = max(s_min,min(s_max, s))
    g_new = s*gâ‚ + (1-s)*g
    return s, g_new
end
quadmin(u, g, d, âˆ‡J, options; kwargs...) = quadmin(u, g, d, âˆ‡J; extractKwargs(options,[:s_guess,:s_max,:print])..., kwargs...)

"""
    armijo(u, J, g, d, Jâˆ‡J; s_guess=1.0, s_decay=0.25, Î²=0.1, max_evals=10, inner=dot, print=0)

Finds s such that Armijo's sufficient descent condition is satisfied, i.e., finds a value of s such that

    Jâˆ‡J(u + s*d) < J + Î²*s*(g â‹… d).

Jâˆ‡J is a function returning the cost functional value as well as the gradient for a given control. J,g must be equal to Jâˆ‡J(u) and are very likely calculated beforehand. These arguments exists such that they do not have to be recalculated inside this function. The dot-product is calculated using the provided Function inner.

Returns the step size s and the costfunction and gradient as given by Jâˆ‡J(u+s*d). If none of the tested values for s results in sufficient descent, returns s=0.0 and J,g (which should equal Jâˆ‡J(u+0*d)). The function Jâˆ‡J is called at least once, and at most max_evals times. The first guess for s is given by s_guess, and subsequent guesses are obtained by multiplying with s_decay each time. Setting print larger than 0 provides more information during the calculation.

For convenience, one may put all, or a subset of, the named arguments above into a struct or named tuple instead.
"""
function armijo(u, J, g, d, Jâˆ‡J; s_guess::Real=1.0, s_decay::Real=0.25, Î²::Float64=0.25, max_evals::Int=10, inner::Function=dot, print::Int=0)
    s_guess <= 0.0 && print>0 && println("s_guess â‰¤ 0, returning 0.0,J,g")
    s_guess <= 0.0 && return 0.0,J,g
    â‹… = inner
    s = s_guess
    local J_new, g_new

    for i = 1:max_evals
        J_new,g_new = Jâˆ‡J(u+s*d)
        # println("i = $i; s = $s; Î” = $(J_new - (J + Î²*s*(g â‹… d))); $(norm(g_new))")
        # check for sufficient descent (Armijo's condition)
        if J_new < J + Î²*s*(g â‹… d)
            print>0 && println("i = $i; s = $s; J(u+s*d) = $J_new < $(J + Î²*s*(g â‹… d)) = J+Î²*s*(gâ‹…d)")
            return s, J_new, g_new
        else
            s*=s_decay
        end
    end

    print>0 && println("No sufficient descent after $max_evals tests of Armijo's condition.
    J(u+s*d) = $J_new > $(J + Î²*s*(g â‹… d))!
    Returning step size 0.")
    print>1 && println("u=$u, g=$g, d=$d")
    return 0.0,J_new,g_new
end
armijo(u, J, g, d, Jâˆ‡J, options; kwargs...) = armijo(u, J, g, d, Jâˆ‡J; extractKwargs(options,[:s_guess,:s_decay,:Î²,:max_evals,:inner,:print])..., kwargs...)

"""
    linesearch(u, J, g, d, Jâˆ‡J, options)

Performs a linesearch with the method given as a string in options.method. "quadmin" leads to [`quadmin`](@ref), "armijo" to [`armijo`](@ref). The options are passed to those methods. If options.method is "constant", then the linesearch step s will always result in s=options.stepsize. Attempts to return values s, J_new, g_new if they can be inferred from the linesearch method without any additional calls of Jâˆ‡J. E.g., J_new is only known when using [`armijo`](@ref) and will be `nothing` when using [`quadmin`](@ref).
"""
function linesearch(u,J,g,d,Jâˆ‡J,options)
    # if :u_norm_bound in propertynames(options)
    #     :u_norm in propertynames(options) || error("if options.u_norm_bound is specified, options.norm_u must also be specified")
    #     s_bound = (max_u_norm - options.norm_u(u))/options.norm_u(d)
    #     # s_bound >= 0 || @warn("Inconsistency during linesearch. The provided u to start from has a larger options.norm_u(u) than options.max_u_norm.")
    #     # s_bound = min(0,s_bound)
    # end
    getd(object,symbol,default) = symbol in propertynames(object) ? getproperty(object,symbol) : default
    s_bound = ( (:s_bound in propertynames(options)) ? options.s_bound(u,d) : Inf)::Float64
    if options.method=="armijo" #TODO make this also work if options.s_guess is not defined.
        if getd(options,:s_guess,1.0) > s_bound
            s,J_new,g_new = armijo(u,J,g,d,Jâˆ‡J,options,s_guess=s_bound)
        else
            s,J_new,g_new = armijo(u,J,g,d,Jâˆ‡J,options)
        end
        return s,J_new,g_new
    elseif options.method=="quadmin"
        âˆ‡J(u) = Jâˆ‡J(u)[2]
        if getd(options,:s_guess,1.0) > s_bound
            s,g_new = quadmin(u,g,d,âˆ‡J,options,s_guess=s_bound)
        else
            s,g_new = quadmin(u,g,d,âˆ‡J,options)
        end
        return s,nothing,g_new
    elseif option.method=="constant"
        # s_bound < linesearch.stepsize || @warn("The constant stepsize results in options.norm_u(u+s*d) > options.norm_u_bound.")
        return min(linesearch.stepsize,s_bound),nothing,nothing
    else
        error("Linesearch method $(options.method) is unknown.")
    end
end

#########################################
## Nonlinear conjugate gradient method ##
#########################################
#TODO: Consistency in the objects generated by the save option.
"""
    ncg(u, f, Ï„, it; norm, print, plotfun, save, ls_options, Î )

NCG method implementation.
# Arguments
- `u`: initial control estimate.
- `f::Function`: function uâ†¦(J(u),âˆ‡J(u)).
- `Ï„`: tolerance on gradient norm.
- `it`: maximum number of iterations.

# Additional optional arguments
- `norm::Function=norm`: The norm to use when reporting â€–uâ€– or â€–âˆ‡Jâ€–
- `print::Int=1`: the amount of information to be printed, with `0` being nothing.
- `plotfun::Function`: if set, `plotfun(k,u,âˆ‡J,d)` is called after every iteration `k`, where `u` is the current iterate, `âˆ‡J` the current gradient and `d` the current search direction. Can be used to plot these quantities during the optimization.
- `save`: provide an empty `Array{Any}` in which iteration information will be stored as tuples `(u,J,âˆ‡J,d)`. The final value of `u` is not stored (but returned).
- `ls_options`: how the step size is determined. A named tuple
    `(method="quadmin",)` (default): Recommended for linear problems
    `(method="armijo",inner,...)`: Recommended for nonlinear problems
        inner should be a function providing the inner product w.r.t. which the gradient is defined. For more options, see [`armijo`](@ref).
- `Î ::Function`: If set, after every linesearch, the current iterate u is projected onto the feasible set by `Î (u)`

# Returns
- Final iterate `u`
"""
function ncg(u, f::Function, Ï„::Real, it::Int; norm::Function=norm, print::Int=1, plotfun::Function=(x...)->nothing, save=false, ls_options=(method="quadmin",), Î =x->x)
    print>0 && println("NCG started.")
    local âˆ‡Jâ‚€,d # Necessary because each for loop iteration allocates new variables.
    J, âˆ‡J = f(u)
    nâˆ‡J = norm(âˆ‡J)
    print>1 && println("i    J              â€–âˆ‡Jâ€–          last step")
    print>1 && @printf("%-3i  %12.10f  %8.6e  (none)\n", 0, J, nâˆ‡J)

    for k = 1:it
        nâˆ‡J<Ï„ && break
        d = k==1 ? -âˆ‡J : getdir(âˆ‡J,âˆ‡Jâ‚€,d) # compute the new search direction
        s,_,_ = linesearch(u,J,âˆ‡J,d,f,ls_options)
        u += s*d
        u = Î (u)
        âˆ‡Jâ‚€ = âˆ‡J
        J, âˆ‡J = f(u)
        nâˆ‡J = norm(âˆ‡J)

        # print, plot, save
        print>1 && @printf("%-3i  %12.10f  %8.6e  %8.2f\n", k, J, nâˆ‡J, s)
        plotfun(k,u,âˆ‡J,d)
        save!=false && push!(save,(u,J,âˆ‡J,d,s))
    end

    print>0 && (nâˆ‡J<Ï„ ? println("NCG successful. Tolerance $Ï„ reached ($nâˆ‡J).") : println("NCG has finished its maximum of $it iterations. Tolerance $Ï„ not reached ($nâˆ‡J)."))
    return u
end

"""
    ncg(u, f, Ï„, it, s; norm, plotfun, print, save, ls_options, Î )

NCG method where `s` is used to specify how `f` should be sampled or otherwise calculated. The provided `f::Function` should behave as follows: `f(u,s)`â†¦`(J,âˆ‡J,ğ”¼y,ğ•y,Ïµ)`. The other arguments have the same meaning as before except for the following:
- `plotfun::Function`: if set, `plotfun(k,u,J,âˆ‡J,ğ”¼y,ğ•y,d)` is called after every iteration `k`, where `u`, `J`, `âˆ‡J` and `d` are as before and `ğ”¼y` and `ğ•y` come from the provided `f`.
- `save`: provide an empty `Array{Any}` in which iteration information will be stored as tuples `(k,u,J,âˆ‡J,ğ”¼y,ğ•y,d,Ïµ,s)`. The final value of `u` is again not stored (but returned)."""
function ncg(u, f::Function, Ï„::Real, iterations::Int, samplingparams; norm::Function=norm, plotfun::Function=(x...)->nothing, print::Int=1, save=false, ls_options=(method="quadmin",), Î =x->x)
    print>0 && println("NCG using fixed samples started.")
    print>1 && println("i    J             Ïµ           â€–âˆ‡Jâ€–          last step")
    âˆ‡Jâ‚€ = zero(u)
    local d # Necessary because each for loop iteration allocates new variables.
    for k = 1:iterations
        J, âˆ‡J, ğ”¼y, ğ•y, Ïµ = f(u,samplingparams)
        nâˆ‡J = norm(âˆ‡J)
        nâˆ‡J<Ï„ && break
        d = k==1 ? -âˆ‡J : getdir(âˆ‡J,âˆ‡Jâ‚€,d) # compute the new search direction
        #println("getdir($(norm(âˆ‡J)),$(norm(âˆ‡Jâ‚€)), d) = $(norm(d))")
        Jâˆ‡J = u->f(u,samplingparams)[1:2]
        s,_,_ = linesearch(u,J,âˆ‡J,d,Jâˆ‡J,ls_options)
        plotfun(k,u,J,âˆ‡J,ğ”¼y,ğ•y,d)
        if save!=false
            push!(save,(k,u,J,âˆ‡J,ğ”¼y,ğ•y,d,s,Ïµ,samplingparams))
        end
        print>1 && @printf("%-3i  %12.10f  %6.4e  %8.6e  %8.2f\n", k, J, Ïµ, nâˆ‡J, s)

        #@printf("iteration %3i. J = %12.10f. Ïµ = %6.4e. â€–âˆ‡Jâ€– = %8.6e. s = %8.2f.\n", k, J, Ïµ, nâˆ‡J, s)

        u += s*d
        u = Î (u)
        âˆ‡Jâ‚€ = âˆ‡J
    end
    return u
end

"""
    ncg(u, f, Ï„, it, q::Real; norm, plotfun, print, save, ls_options, Î )

NCG method taking into account the accuracy of the gradient. `q` provides the bound for the relative RMSE on the gradient. The provided function `f` must provide 2 methods:
- `f(u,s)`â†¦`(J,âˆ‡J,ğ”¼y,ğ•y,Ïµ::Real)` to obtain (amongst other things) the RMSE `Ïµ` given the sampling method `s`.
- `f(u,Ïµ::Real)`â†¦`(J,âˆ‡J,ğ”¼y,ğ•y,s)` to obtain (amongst other things) a sampling method `s` that attains a RMSE of at most `Ïµ`.
Here `s` can obviously not be subtype of `Real`. The additional arguments have the same meaning as before."""
function ncg(u, f::Function, Ï„::Real, iterations::Int, q::Real; norm::Function=norm, print::Int=1, plotfun::Function=(x...)->nothing, save=false, ls_options=(method="quadmin",),Î =x->x)
    print>0 && println("NCG started.")
    local d, âˆ‡Jâ‚€ # Necessary because each for loop iteration allocates new variables.
    Ïµ = 0.01 # first requested Ïµ
    Î· = 0.25 # decay of requested Ïµ
    #q = 1.0 # bound for the relative RMSE on gradient
    J, âˆ‡J, ğ”¼y, ğ•y, samplingdata = f(u,Ïµ);
    save!=false && push!(save,(u,J,âˆ‡J,ğ”¼y,ğ•y,nothing,nothing,Ïµ,samplingdata,0.0))
    print>1 && println("New n = $(samplingdata.n), for requested Ïµ = $(@sprintf("%6.4e", Ïµ)).")
    print>1 && println("i    J             Ïµ           â€–âˆ‡Jâ€–          last step")
    for k = 1:iterations
        tstart = time();
        if norm(âˆ‡J)<=Ï„
            print>1 && @printf("Fixed sample gradient is %8.6e. Testing â€–âˆ‡Jâ€– for convergence using new samples", norm(âˆ‡J))
            tested = norm(f(u,Ïµ)[2])
            print>1 && @printf(": %8.6e.\n", tested)
            if tested <=Ï„
                return u
            end
        end
        d = k==1 ? -âˆ‡J : getdir(âˆ‡J,âˆ‡Jâ‚€,d) # compute the new search direction
        Jâˆ‡J = u->f(u,samplingdata)[1:2]
        s,_,_ = linesearch(u,J,âˆ‡J,d,Jâˆ‡J,ls_options)
        u += s*d
        u = Î (u)
        âˆ‡Jâ‚€ = âˆ‡J
        nâˆ‡J = norm(âˆ‡J)
        if Ïµ > max(q*Ï„,q*nâˆ‡J) || Ïµ < Î·^2*q*nâˆ‡J
            print>1 && @printf("Change of sample set (Ïµ = %6.4e, â€–âˆ‡Jâ€– = %8.6e).\n", Ïµ, nâˆ‡J)
            Ïµ = max(q*Ï„,Î·*q*nâˆ‡J)
            J, âˆ‡J, ğ”¼y, ğ•y, samplingdata = f(u,Ïµ);
            print>1 && Base.print("New n = $(samplingdata.n), for requested Ïµ = $(@sprintf("%6.4e", Ïµ)).\n")
        else
            J, âˆ‡J, ğ”¼y, ğ•y, Ïµ = f(u,samplingdata)
        end
        print>1 && @printf("%-3i  %12.10f  %6.4e  %8.6e  %8.2f\n", k, J, Ïµ, nâˆ‡J, s)

        plotfun(k,u,J,âˆ‡J,ğ”¼y,ğ•y,d)
        save!=false && push!(save,(u,J,âˆ‡J,ğ”¼y,ğ•y,d,s,Ïµ,samplingdata,time()-tstart))
    end
    return u
end

############
## MG/OPT ##
############

"""
    gen_f(cs::Vector{ComputeStruct}, ss::Vector{SamplingData})

Generates function returning (J,âˆ‡J) for a given Vector of ComputeStruct and a Vector of SamplingData (of which elements k contains the ComputeStruct and SamplingData for MG/OPT level k)."""
function gen_f(cs::Vector{ComputeStruct}, ss::Vector{SamplingData})
    function f(u,â„“)
        c = cs[â„“+1]
        s = ss[â„“+1]
        c(u,s)[1:2]
    end
end

"""
    smooth(uâ‚€,f,Î¼[;breakcond,ls_options])

NCG smoother using Î¼ smoothing steps. Returns new control iterate u, gradient in u, gradient in uâ‚€ and number of NCG steps that were performed (equal to Î¼ if no breakcond is set (see below)).

# Arguments
- `u`: current control iterate.
- `f::Function`: function uâ†¦(J(u),âˆ‡J(u)).
- `Î¼::Int`: number of smoothing steps.

# Additional arguments
- `breakcond::Function`: breaks the smoothing once breakcond(âˆ‡J(u)) is satisfied.
- `ls_options`: how the step size is determined. A named tuple
    (method="quadmin",) (default): Recommended for linear problems
    (method="armijo",inner,...): Recommended for nonlinear problems
        inner should be a function providing the inner product w.r.t. which the gradient is defined. For more options, see [`armijo`](@ref).
"""
function smooth(u,f::Function,Î¼::Int;print=1,norm_âˆ‡J=norm,breakcond=g->false,ls_options=(method="quadmin",),ls_options_fallback=(method="none",))
    local d # Necessary because each for loop iteration allocates new variables.
    #if !state; âˆ‡J_prev,d = state; end
    i = 0
    J,âˆ‡J = f(u)
    Jâ‚€,âˆ‡Jâ‚€ = J,âˆ‡J # save starting costfun and gradient
    u_prev,J_prev,âˆ‡J_prev = u,J,âˆ‡J
 #    print>0 && println(" | smooth($Î¼):
 # |       ls_options = $ls_options
 # |       ls_options_fallback = $ls_options_fallback")
    print>0 && println(" | smooth($Î¼):")
    for outer i = 1:Î¼
        d = i==1 ? -âˆ‡J : getdir(âˆ‡J,âˆ‡J_prev,d) # compute the new search direction
        s,_,_ = linesearch(u,J,âˆ‡J,d,f,ls_options)

        linesearch_fail = false
        s_bound = ( (:s_bound in propertynames(ls_options)) ? ls_options.s_bound(u,d) : Inf)::Float64
        if s < 0 || s > s_bound # Check the admissibility of s (and therefore the next iterate u)
            linesearch_fail = true
            print>0 && println(" |  ($i) J = $J, â€–âˆ‡Jâ€– = $(norm_âˆ‡J(âˆ‡J)), s=$s (s is not admissible)")
        else
            u_try = u + s*d
            J_try,âˆ‡J_try = f(u_try) # NOTE: For quadratic problems, one may obtain the gradient at the new point from the quadmin linesearch for free.
            if J_try >= J # Checks descent
                linesearch_fail = true
                print>0 && println(" |  ($i) J = $J, â€–âˆ‡Jâ€– = $(norm_âˆ‡J(âˆ‡J)), s=$s (no descent: next J would be $J_try)")
                # d = -âˆ‡J_prev
            end
        end

        if linesearch_fail && ls_options_fallback.method!="none"
            print>0 && println(" |       Trying a linesearch specified by ls_options_fallback.")
            # d = -âˆ‡J_prev
            s,_,_ = linesearch(u,J,âˆ‡J,d,f,ls_options_fallback)
            u_try = u + s*d
            J_try,âˆ‡J_try = f(u_try)
            if J_try >= J
                print>0 && println(" |       Still no descent: next J would be $J_try. Trying again with search direction -âˆ‡J.")
                d = -âˆ‡J_prev
                s,_,_ = linesearch(u,J,âˆ‡J,d,f,ls_options_fallback)
                u_try = u + s*d
                J_try,âˆ‡J_try = f(u_try)
                if J_try >= J
                    #throw(DebugException("No descent", (u=u,âˆ‡J=âˆ‡J_prev)))
                    print>0 && println(" |       WARNING! No descent. next J WILL be $(J_try)!")
                end
            end
        end

        print>0 && println(" |  ($i) J = $J, â€–âˆ‡Jâ€– = $(norm_âˆ‡J(âˆ‡J)), s=$s")

        # Finalize update
        u_prev,J_prev,âˆ‡J_prev = u,J,âˆ‡J
        u,J,âˆ‡J = u_try,J_try,âˆ‡J_try

        if breakcond(âˆ‡J); break; end
    end
    #DEBUG println(" | SMOOTHER DEBUG: Jâ‚€ = $Jâ‚€ and J = $J and Jâ‚€-J = $(Jâ‚€-J)")
    return u, J, âˆ‡J, Jâ‚€, âˆ‡Jâ‚€, i
end

"""
    mgoptV(u,v,f,I,k[,K])

Implementation of MG/OPT V-cycle.

# Arguments
- `u`: initial guess.
- `v`: corrective term.
- `f::Function`: function for calculating (J,âˆ‡J) given control u (discretized at level k) and MG/OPT level k.
- `I::Function`: level mapping function (u,k) â†¦ u mapped to level k.
- `k::Int`: current MG/OPT level.
- `Î¼_pre::Vector{Int}`: Î¼_pre[k] gives the number of presmoothing steps at level k.
- `Î¼_post::Vector{Int}`: Î¼_post[k] gives the number of postsmoothing steps at level k.
- `K::Int`: finest MG/OPT level (defaults to k).

# Additional arguments
- `smooth::Function=smooth`: The smoother used. Should be callable as smooth(f,uâ‚€,Î¼) with f(u)â†¦(J(u),âˆ‡J(u)), uâ‚€ the starting value in the iteration and Î¼ the number of smoothing steps.
- `inner::Function=dot`: The inner product w.r.t. which the gradient is defined.
- `print::Bool=false`: set to `true` to give some information
- `kmin::Int=0`: The depth that the V-cycle should reach.
- `ls_options`: how the step size is determined. A named tuple
    (method="constant",stepsize=1.0) (default): Recommended close to the solution
    (method="quadmin",): Recommended for linear problems far away from the solution.
    (method="armijo",inner,...): Recommended for nonlinear problems
        inner should be a function providing the inner product w.r.t. which the gradient is defined. For more options, see [`armijo`](@ref).

# Returns
- approximation of minimizer of f(u)[1] - vâ‹…u
- Costfunction at that approximation
- Gradient at that approximation
- Costfunction at the start of the V-cycle
- Gradient at the start of the V-cycle
"""
function mgoptV(u,v,f::Function,lm::Function,k::Int, Î¼_pre::Vector{Int} ,Î¼_post::Vector{Int}, K::Int=k; smooth::Function=(u,f,Î¼,k)->smooth(u,f,Î¼), inner::Function=dot, print::Int=0, kmin::Int=0, ls_options=(method="constant",stepsize=1.0))
    DEBUG = false

    norm(x) = sqrt(inner(x,x))
    â‹… = inner

    # function modified by v to optimize
    function fv(u)
        J1,âˆ‡J1 = f(u,k)
        Jv = J1 - vâ‹…u
        âˆ‡Jv = âˆ‡J1 - v
        return Jv,âˆ‡Jv
    end

    print>0 && k==K && println("MG/OPT started. Î¼_pre = $Î¼_pre. Î¼_post = $Î¼_post.")

    if k<=kmin #trivial case
        # NOTE: One may also solve the optimization problem with ğ”¼[k(x,Ï‰)] as system parameters
        # or do a full Newton step since the Hessian will now be small anyway.
        DEBUG && println(" | debug: u=$u, v=$v")
        u,J,âˆ‡J,Jâ‚€,âˆ‡Jâ‚€,Î¼done=smooth(u,fv,Î¼_pre[k+1]+Î¼_post[k+1],k)
        print>0 && println(" | k = $k: After solve($Î¼done): â€–âˆ‡Jâ€– = $(norm(âˆ‡J))")
        return u,J,âˆ‡J,Jâ‚€,âˆ‡Jâ‚€
    end

    # pre-smoothing
    print>0 && println(" | k = $k: Presmoothing...")
    u,J,âˆ‡J,Jâ‚€,âˆ‡Jâ‚€,Î¼done=smooth(u,fv,Î¼_pre[k+1],k) #NOTE: J,âˆ‡J here contains -v part, since fv is supplied!!!
    print>0 && println(" | k = $k: After presmoothing($Î¼done): â€–âˆ‡Jâ€– = $(norm(âˆ‡J))")
    DEBUG && println(" | MG/OPT DEBUG: Jâ‚€=$Jâ‚€")

    # recursion
    uc = lm(u,k-1)
    _,âˆ‡Jc = f(uc,k-1) #NOTE: In theory free, but cost negligible compared to fine gradient
    J,âˆ‡J = f(u,k) #NOTE: should be obtained in smoothing step
    vc = lm(v,k-1) + âˆ‡Jc - lm(âˆ‡J,k-1) #NOTE: This last correction can in theory come for free
    uc_new = mgoptV(uc,vc,f,lm,k-1,Î¼_pre,Î¼_post,K; smooth=smooth, inner=inner, print=print, kmin=kmin, ls_options=ls_options)[1]
    ec = uc_new - uc
    e = lm(ec,k)

    # linesearch
    s,_,_ = linesearch(u,J-vâ‹…u,âˆ‡J-v,e,fv,ls_options)
    print>1 && println(" | k = $k: step = $s")

    u = u + s*e

    # post-smoothing
    print>0 && println(" | k = $k: Postsmoothing...")
    # try
    #    u,J,âˆ‡J,J_debug,âˆ‡J_debug,Î¼done=smooth(u,fv,Î¼_post[k+1],k)
    #catch e
    #    throw(DebugException(e,"",(u=u,v=v)))
    #end
    u,J,âˆ‡J,J_debug,âˆ‡J_debug,Î¼done=smooth(u,fv,Î¼_post[k+1],k)
    print>0 && println(" | k = $k: After postsmoothing($Î¼done): â€–âˆ‡Jâ€– = $(norm(âˆ‡J))")
    DEBUG && println(" | MG/OPT DEBUG: Jâ‚€=$Jâ‚€, J_debug=$J_debug, J=$J, ")
    DEBUG && println(" | Jâ‚€-J_debug = $(Jâ‚€-J_debug) and J_debug-J = $(J_debug-J)")
    return u,J,âˆ‡J,Jâ‚€,âˆ‡Jâ‚€
end

"""
    mgoptW(u,v,f,I,k[,K])

Implementation of MG/OPT W-cycle.

# Arguments
- `u`: initial guess.
- `v`: corrective term.
- `f::Function`: function for calculating (J,âˆ‡J) given control u (discretized at level k) and MG/OPT level k.
- `I::Function`: level mapping function (u,k) â†¦ u mapped to level k.
- `k::Int`: current MG/OPT level.
- `Î¼_pre::Vector{Int}`: Î¼_pre[k] gives the number of presmoothing steps at level k.
- `Î¼_post::Vector{Int}`: Î¼_post[k] gives the number of postsmoothing steps at level k.
- `K::Int`: finest MG/OPT level (defaults to k).

# Additional arguments
- `smooth::Function=smooth`: The smoother used. Should be callable as smooth(f,uâ‚€,Î¼) with f(u)â†¦(J(u),âˆ‡J(u)), uâ‚€ the starting value in the iteration and Î¼ the number of smoothing steps.
- `inner::Function=dot`: The inner product w.r.t. which the gradient is defined.
- `print::Bool=false`: set to `true` to give some information
- `kmin::Int=0`: The depth that the W-cycle should reach.
- `ls_options`: how the step size is determined. A named tuple
    (method="constant",stepsize=1.0) (default): Recommended close to the solution
    (method="quadmin",): Recommended for linear problems far away from the solution.
    (method="armijo",inner,...): Recommended for nonlinear problems
        inner should be a function providing the inner product w.r.t. which the gradient is defined. For more options, see [`armijo`](@ref).

# Returns
- approximation of minimizer of f(u)[1] - vâ‹…u
- Costfunction at that approximation
- Gradient at that approximation
- Costfunction at the start of the W-cycle
- Gradient at the start of the W-cycle
"""
function mgoptW(u,v,f::Function,lm::Function,k::Int, Î¼_pre::Vector{Int} ,Î¼_post::Vector{Int}, K::Int=k; smooth::Function=(u,f,Î¼,k)->smooth(u,f,Î¼), inner::Function=dot, print::Int=0, kmin::Int=0, ls_options=(method="constant",stepsize=1.0))
    DEBUG = false

    norm(x) = sqrt(inner(x,x))
    â‹… = inner

    # function modified by v to optimize
    function fv(u)
        J1,âˆ‡J1 = f(u,k)
        Jv = J1 - vâ‹…u
        âˆ‡Jv = âˆ‡J1 - v
        return Jv,âˆ‡Jv
    end

    print>0 && k==K && println("MG/OPT started. Î¼_pre = $Î¼_pre. Î¼_post = $Î¼_post.")

    if k<=kmin #trivial case
        # NOTE: One may also solve the optimization problem with ğ”¼[k(x,Ï‰)] as system parameters
        # or do a full Newton step since the Hessian will now be small anyway.
        DEBUG && println(" | debug: u=$u, v=$v")
        u,J,âˆ‡J,Jâ‚€,âˆ‡Jâ‚€,Î¼done=smooth(u,fv,2Î¼_pre[k+1]+2Î¼_post[k+1],k) #NOTE: factor 2 here
        print>0 && println(" | k = $k: After solve($Î¼done): â€–âˆ‡Jâ€– = $(norm(âˆ‡J))")
        return u,J,âˆ‡J,Jâ‚€,âˆ‡Jâ‚€
    end

    # pre-smoothing
    print>0 && println(" | k = $k: Presmoothing...")
    u,J,âˆ‡J,Jâ‚€,âˆ‡Jâ‚€,Î¼done=smooth(u,fv,Î¼_pre[k+1],k) #NOTE: J,âˆ‡J here contains -v part, since fv is supplied!!!
    print>0 && println(" | k = $k: After presmoothing($Î¼done): â€–âˆ‡Jâ€– = $(norm(âˆ‡J))")
    DEBUG && println(" | MG/OPT DEBUG: Jâ‚€=$Jâ‚€")

    # recursion
    uc = lm(u,k-1)
    _,âˆ‡Jc = f(uc,k-1) #NOTE: In theory free, but cost negligible compared to fine gradient
    J,âˆ‡J = f(u,k) #NOTE: should be obtained in smoothing step
    vc = lm(v,k-1) + âˆ‡Jc - lm(âˆ‡J,k-1) #NOTE: This last correction can in theory come for free
    if k<=kmin+1 #W-cycle below will be trivial case
        uc_new = mgoptW(uc,vc,f,lm,k-1,Î¼_pre,Î¼_post,K; smooth=smooth, inner=inner, print=print, kmin=kmin, ls_options=ls_options)[1]
    else
        Î¼_pre1 = Î¼_pre[1:k]; Î¼_post1 = [Î¼_post[1:k-1];2*Î¼_post[k]]; #additional level k post smoothing here
        uc_new = mgoptW(uc,vc,f,lm,k-1,Î¼_pre1,Î¼_post1,K; smooth=smooth, inner=inner, print=print, kmin=kmin, ls_options=ls_options)[1]
        Î¼_pre2 = [Î¼_pre[1:k-1];0]; Î¼_post2 = Î¼_post[1:k]; #reduced level k pre smoothing here
        uc_new = mgoptW(uc_new,vc,f,lm,k-1,Î¼_pre2,Î¼_post2,K; smooth=smooth, inner=inner, print=print, kmin=kmin, ls_options=ls_options)[1]
    end
    ec = uc_new - uc
    e = lm(ec,k)

    # linesearch
    s,_,_ = linesearch(u,J-vâ‹…u,âˆ‡J-v,e,fv,ls_options)
    print>1 && println(" | k = $k: step = $s")

    u = u + s*e

    # post-smoothing
    print>0 && println(" | k = $k: Postsmoothing...")
    # try
    #    u,J,âˆ‡J,J_debug,âˆ‡J_debug,Î¼done=smooth(u,fv,Î¼_post[k+1],k)
    #catch e
    #    throw(DebugException(e,"",(u=u,v=v)))
    #end
    u,J,âˆ‡J,J_debug,âˆ‡J_debug,Î¼done=smooth(u,fv,Î¼_post[k+1],k)
    print>0 && println(" | k = $k: After postsmoothing($Î¼done): â€–âˆ‡Jâ€– = $(norm(âˆ‡J))")
    DEBUG && println(" | MG/OPT DEBUG: Jâ‚€=$Jâ‚€, J_debug=$J_debug, J=$J, ")
    DEBUG && println(" | Jâ‚€-J_debug = $(Jâ‚€-J_debug) and J_debug-J = $(J_debug-J)")
    return u,J,âˆ‡J,Jâ‚€,âˆ‡Jâ‚€
end

end



# # W-cycle test
# if k<=kmin+1
#     uc_new = mgoptW(uc,vc,f,lm,k-1,Î¼_pre[1:1],Î¼_post[1:1],K; smooth=smooth, inner=inner, print=print, kmin=kmin, ls_options=ls_options)[1]
# else
#     Î¼_pre1 = Î¼_pre[1:k]; Î¼_post1 = [Î¼_post[1:k-1];2*Î¼_post[k]];
#     uc_new = mgoptW(uc,vc,f,lm,k-1,Î¼_pre1,Î¼_post1,K; smooth=smooth, inner=inner, print=print, kmin=kmin, ls_options=ls_options)[1]
#     Î¼_pre2 = [Î¼_pre[1:k-1];0]; Î¼_post2 = Î¼_post[1:k];
#     uc_new = mgoptW(uc_new,vc,f,lm,k-1,Î¼_pre2,Î¼_post2,K; smooth=smooth, inner=inner, print=print, kmin=kmin, ls_options=ls_options)[1]
# end
